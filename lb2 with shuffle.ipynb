{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим датасет и проверим, соответствует ли он требованиям длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общий объём текста: 3493493 символов, 586661 слов.\n",
      "0    я учусь в восьмом классе\n",
      "1    я считаю себя изучателем\n",
      "2              План не сложен\n",
      "Name: expanded_phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"inkoziev/incomplete_utterance_restoration\")\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "\n",
    "text_column = df.select_dtypes(include=['object']).columns[0]\n",
    "\n",
    "text = df[text_column]\n",
    "total_chars = text.str.len().sum()\n",
    "total_words = text.str.split().str.len().sum()\n",
    "\n",
    "print(f\"Общий объём текста: {total_chars} символов, {total_words} слов.\")\n",
    "print(text.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный датасет имеет достаточно слов для дальнейшего обучения НС. Продолжим работу с датасетом - разделим предложения на слова и создадим словарь слов и их индексов для быстрого поиска.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['я', 'учусь', 'в', 'восьмом', 'классе'], ['я', 'считаю', 'себя', 'изучателем'], ['План', 'не', 'сложен'], ['я', 'неплохо', 'поживаю'], ['Смертен', 'ли', 'Гиппарх?']]\n",
      "['музыкальный', '1964', 'конструированием', 'присущ', 'дверь,', '100*86=8600', 'Литератор', 'радоваться,', 'счетчика', 'бабелевские'] 71663\n",
      "18614\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "L = 6\n",
    "sentences = [sentence.split() for sentence in text]\n",
    "print(sentences[:5])\n",
    "\n",
    "# поделим предложения на уникальные слова\n",
    "for sentence in sentences:\n",
    "    vocab.update(sentence)\n",
    "\n",
    "vocab = list(vocab)\n",
    "print(vocab[:10], len(vocab))\n",
    "\n",
    "word_indx = {w: i for i, w in enumerate(vocab)}\n",
    "print(word_indx[\"я\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим положительные (реальные пары) и отрицательные примеры (случайные пары):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 'учусь'), ('я', 'в'), ('я', 'восьмом'), ('я', 'классе'), ('учусь', 'я'), ('учусь', 'в'), ('учусь', 'восьмом'), ('учусь', 'классе'), ('в', 'я'), ('в', 'учусь'), ('в', 'восьмом'), ('в', 'классе'), ('восьмом', 'я'), ('восьмом', 'учусь'), ('восьмом', 'в')]\n"
     ]
    }
   ],
   "source": [
    "positiv_pairs = []\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        target = sentence[i]\n",
    "        for j in range(max(0, i - L), min(i + L + 1, len(sentence))):\n",
    "            if i != j:\n",
    "                positiv_pairs.append((target, sentence[j]))\n",
    "\n",
    "print(positiv_pairs[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 'верующих'), ('я', 'Морковь'), ('я', 'поэбатле'), ('я', 'боженька,'), ('учусь', 'рукав'), ('учусь', '3-4'), ('учусь', 'тренировке'), ('учусь', 'Карамазовы\",'), ('в', 'килограммов'), ('в', 'крематорию'), ('в', 'сакральный'), ('в', 'эстафеты'), ('восьмом', 'радости?'), ('восьмом', 'честь.'), ('восьмом', 'черепахах')]\n"
     ]
    }
   ],
   "source": [
    "negative_pairs = []\n",
    "for target, context in positiv_pairs:\n",
    "    neg_context = random.choice(vocab)\n",
    "    while neg_context == context:\n",
    "        neg_context = random.choice(vocab)\n",
    "    negative_pairs.append((target, neg_context))\n",
    "\n",
    "print(negative_pairs[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим сбалансированный датасет, состоящий как из положительных, так и из отрицательных примеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18614  9118]\n",
      " [18614  5128]\n",
      " [18614 38686]\n",
      " [18614 51794]\n",
      " [ 9118 18614]] [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [] # пары (индексы пар)\n",
    "Y = [] # метки (для реальных пар 1, для случайных 0)\n",
    "\n",
    "for target, contex in positiv_pairs:\n",
    "    X.append([word_indx[target], word_indx[contex]])\n",
    "    Y.append(1)\n",
    "\n",
    "for target, neg_context in negative_pairs:\n",
    "    X.append([word_indx[target], word_indx[neg_context]])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(X[:5], Y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс Word2Vec для реализации эмбедингов и обучения и создадим и обучим полносвязную нейронную сеть с одним слоем (dh = 500):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение модели с длиной эмбединга 100\n",
      "Обучение W2V\n",
      "Эпоха: 0, ошибка: 0.6930\n",
      "Эпоха: 1, ошибка: 0.6743\n",
      "Эпоха: 2, ошибка: 0.6341\n",
      "Эпоха: 3, ошибка: 0.6085\n",
      "Эпоха: 4, ошибка: 0.5892\n",
      "Эпоха: 5, ошибка: 0.5720\n",
      "Эпоха: 6, ошибка: 0.5553\n",
      "Слова, похожие на 'король':\n",
      "посещение (сходство: 0.986)\n",
      "кормлю (сходство: 0.986)\n",
      "родился (сходство: 0.986)\n",
      "найдешь (сходство: 0.985)\n",
      "должны (сходство: 0.985)\n",
      "потерял (сходство: 0.985)\n",
      "закажу (сходство: 0.985)\n",
      "проявляется, (сходство: 0.985)\n",
      "живой (сходство: 0.985)\n",
      "отдыхать (сходство: 0.985)\n",
      "Создание и обучение нейросети\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksenia/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.0213 - loss: 10.1051 - val_accuracy: 0.0538 - val_loss: 8.6541\n",
      "Epoch 2/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.0652 - loss: 7.3442 - val_accuracy: 0.0798 - val_loss: 8.6346\n",
      "Epoch 3/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.0860 - loss: 6.3433 - val_accuracy: 0.1016 - val_loss: 9.1119\n",
      "Epoch 4/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.1082 - loss: 5.6410 - val_accuracy: 0.1234 - val_loss: 9.7235\n",
      "Epoch 5/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.1308 - loss: 5.0670 - val_accuracy: 0.1432 - val_loss: 10.3966\n",
      "Epoch 6/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.1702 - loss: 4.4959 - val_accuracy: 0.1569 - val_loss: 10.9319\n",
      "Epoch 7/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.2297 - loss: 3.9671 - val_accuracy: 0.1714 - val_loss: 11.5625\n",
      "Epoch 8/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.2991 - loss: 3.4392 - val_accuracy: 0.1880 - val_loss: 12.2717\n",
      "Epoch 9/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.3857 - loss: 2.9264 - val_accuracy: 0.2007 - val_loss: 12.8058\n",
      "Epoch 10/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.4695 - loss: 2.4424 - val_accuracy: 0.2169 - val_loss: 13.3691\n",
      "Epoch 11/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.5502 - loss: 2.0194 - val_accuracy: 0.2277 - val_loss: 13.9154\n",
      "Epoch 12/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 1s/step - accuracy: 0.6151 - loss: 1.6684 - val_accuracy: 0.2357 - val_loss: 14.3553\n",
      "Epoch 13/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.6809 - loss: 1.3706 - val_accuracy: 0.2466 - val_loss: 14.7059\n",
      "Epoch 14/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.7299 - loss: 1.1381 - val_accuracy: 0.2501 - val_loss: 15.0577\n",
      "Epoch 15/15\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.7764 - loss: 0.9362 - val_accuracy: 0.2596 - val_loss: 15.2738\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=model_100.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 122\u001b[0m\n\u001b[1;32m    112\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    113\u001b[0m     X, y, \n\u001b[1;32m    114\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[1;32m    119\u001b[0m )\n\u001b[1;32m    121\u001b[0m models[d] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 122\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43md\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m histories[d] \u001b[38;5;241m=\u001b[39m history\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/saving/saving_api.py:114\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    112\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    113\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=model_100."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.models\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def show_similar_words(emb_matrix, word_indx, vocab, word, top_n=10):\n",
    "    if word not in word_indx:\n",
    "        print(f\"Слово '{word}' не найдено в словаре.\")\n",
    "        return\n",
    "    vec = emb_matrix[word_indx[word]].reshape(1, -1)\n",
    "    similarities = cosine_similarity(vec, emb_matrix)[0]\n",
    "    \n",
    "    # отсортируем и выведем top_n похожих слов\n",
    "    similar_ids = np.argsort(-similarities)[:top_n + 1]\n",
    "    print(f\"Слова, похожие на '{word}':\")\n",
    "    for idx in similar_ids:\n",
    "        if vocab[idx] != word:\n",
    "            print(f\"{vocab[idx]} (сходство: {similarities[idx]:.3f})\")\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embedding_dim, lambda_coef=0.001):\n",
    "        self.W = np.random.randn(vocab_size, embedding_dim) * lambda_coef\n",
    "        self.C = np.random.randn(vocab_size, embedding_dim) * lambda_coef\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -50, 50)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, positiv_pairs, negative_pairs, learning_rate=0.0005, epochs=7):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for target, context in positiv_pairs:\n",
    "                target_ind = word_indx[target]\n",
    "                context_ind = word_indx[context]\n",
    "\n",
    "                score = np.dot(self.W[target_ind], self.C[context_ind]) \n",
    "                prob = max(self.sigmoid(score), 1e-10)\n",
    "                loss = -np.log(prob)\n",
    "                total_loss += loss\n",
    "\n",
    "                grad = (1 - prob) * learning_rate\n",
    "                self.W[target_ind] += grad  * self.C[context_ind]\n",
    "                self.C[context_ind] += grad * self.W[target_ind]\n",
    "\n",
    "            for target, neg_context in negative_pairs:\n",
    "                target_ind = word_indx[target]\n",
    "                neg_ind = word_indx[neg_context]\n",
    "\n",
    "                score = np.dot(self.W[target_ind], self.C[neg_ind])\n",
    "                prob = self.sigmoid(-score)\n",
    "                loss = -np.log(prob)\n",
    "                total_loss += loss\n",
    "\n",
    "                grad = -prob * learning_rate\n",
    "                self.W[target_ind] += grad * self.C[neg_ind]\n",
    "                self.C[neg_ind] += grad * self.W[target_ind]\n",
    "            \n",
    "            print(f\"Эпоха: {epoch}, ошибка: {total_loss/len(positiv_pairs + negative_pairs):.4f}\")\n",
    "        \n",
    "        return self.W\n",
    "\n",
    "# подготовка данных для нейросети\n",
    "def prepare_data(sentences, L):\n",
    "    X, y = [], []\n",
    "    for sentence in sentences:\n",
    "        for i in range(L, len(sentence)):\n",
    "            context = sentence[i-L:i]\n",
    "            target = sentence[i]\n",
    "            if all(word in word_indx for word in context) and target in word_indx:\n",
    "                X.append([word_indx[word] for word in context])\n",
    "                y.append(word_indx[target])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "dh = 500\n",
    "embedding_d = [100, 500, 1000]\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "for d in embedding_d:\n",
    "    print(f\"Обучение модели с длиной эмбединга {d}\")\n",
    "    \n",
    "    print(\"Обучение W2V\")\n",
    "    w2v = Word2Vec(len(vocab), d)\n",
    "    emb_matrix = w2v.train(positiv_pairs, negative_pairs)\n",
    "    \n",
    "    show_similar_words(emb_matrix, word_indx, vocab, \"король\")\n",
    "\n",
    "    X, y = prepare_data(sentences, L)\n",
    "    print(\"Создание и обучение нейросети\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(len(vocab), d, weights=[emb_matrix], input_length=L, trainable=True),\n",
    "        Flatten(),\n",
    "        Dense(dh, activation='relu'),\n",
    "        Dense(len(vocab), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        patience=2, \n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X, y, \n",
    "        epochs=15,\n",
    "        shuffle=True,\n",
    "        batch_size=512,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    models[d] = model\n",
    "\n",
    "    histories[d] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем нейронную сеть на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mОшибка предсказания: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m test_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mкаждый день я\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 54\u001b[0m predict_next_word(\u001b[43mmodels\u001b[49m, vocab, word_indx, test_sentence, L, embedding_d)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_next_word(models, vocab, word_indx, test_sentence, L, embedding_d):\n",
    "    # токенизация и подготовка контекста\n",
    "    test_words = test_sentence.split()\n",
    "    \n",
    "    # проверка, что контекст достаточной длины\n",
    "    if len(test_words) < L:\n",
    "        print(f\"Ошибка: предложение слишком короткое. Нужно минимум {L} слов, а получено {len(test_words)}\")\n",
    "        return\n",
    "    \n",
    "    # Бебрём последние L слов как контекст\n",
    "    context = test_words[-L:]\n",
    "    print(f\"\\nТестовый контекст: {' '.join(context)}\")\n",
    "    \n",
    "    # преобразуем слова в индексы с обработкой неизвестных слов\n",
    "    test_indices = []\n",
    "    unk_token = \"<UNK>\"\n",
    "    \n",
    "    for word in context:\n",
    "        if word in word_indx:\n",
    "            test_indices.append(word_indx[word])\n",
    "        else:\n",
    "            print(f\"Слово '{word}' не найдено в словаре. Заменяю на {unk_token}\")\n",
    "            if unk_token in word_indx:\n",
    "                test_indices.append(word_indx[unk_token])\n",
    "            else:\n",
    "                print(f\"Ошибка: токен {unk_token} отсутствует в словаре\")\n",
    "                return\n",
    "    \n",
    "    # предсказание для каждой модели\n",
    "    for d in embedding_d:\n",
    "        if d not in models:\n",
    "            print(f\"Модель с d={d} не найдена\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nПредсказание для модели с d={d}:\")\n",
    "        test_X = np.array([test_indices])\n",
    "        \n",
    "        try:\n",
    "            pred = models[d].predict(test_X, verbose=0)\n",
    "            predicted_idx = np.argmax(pred[0])\n",
    "            predicted_word = vocab[predicted_idx]\n",
    "            print(f\"Следующее слово: '{predicted_word}' (вероятность: {np.max(pred[0]):.2%})\")\n",
    "            \n",
    "            # Топ-5 вариантов\n",
    "            top_indices = np.argsort(-pred[0])[:5]\n",
    "            print(\"Топ-5 вариантов:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1}. {vocab[idx]} ({pred[0][idx]:.2%})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка предсказания: {str(e)}\")\n",
    "\n",
    "test_sentence = \"я учусь в восьмом классе я считаю себя\"\n",
    "predict_next_word(\n",
    "    {100: tensorflow.keras.models.load_model(\"model_100\"), 500: tensorflow.keras.models.load_model(\"model_500\"), 1000: tensorflow.keras.models.load_model(\"model_1000\")},\n",
    "    vocab, word_indx, test_sentence, L, [100, 500, 1000])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
