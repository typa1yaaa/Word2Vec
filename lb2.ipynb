{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим датасет и проверим, соответствует ли он требованиям длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 123930/123930 [00:01<00:00, 62308.73 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общий объём текста: 3493493 символов, 586661 слов.\n",
      "0    я учусь в восьмом классе\n",
      "1    я считаю себя изучателем\n",
      "2              План не сложен\n",
      "Name: expanded_phrase, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"inkoziev/incomplete_utterance_restoration\")\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "\n",
    "text_column = df.select_dtypes(include=['object']).columns[0]\n",
    "\n",
    "text = df[text_column]\n",
    "total_chars = text.str.len().sum()\n",
    "total_words = text.str.split().str.len().sum()\n",
    "\n",
    "print(f\"Общий объём текста: {total_chars} символов, {total_words} слов.\")\n",
    "print(text.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный датасет имеет достаточно слов для дальнейшего обучения НС. Продолжим работу с датасетом - разделим предложения на слова и создадим словарь слов и их индексов для быстрого поиска.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['я', 'учусь', 'в', 'восьмом', 'классе'], ['я', 'считаю', 'себя', 'изучателем'], ['План', 'не', 'сложен'], ['я', 'неплохо', 'поживаю'], ['Смертен', 'ли', 'Гиппарх?']]\n",
      "['вчера?', 'балалайку', 'яхт?', 'тоже?', 'Макаром', 'Мартин.', 'офигеваю', 'Уилла', 'мойву', 'крышку'] 71663\n",
      "61313\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "vocab = set()\n",
    "\n",
    "L = 5\n",
    "sentences = [sentence.split() for sentence in text]\n",
    "print(sentences[:5])\n",
    "\n",
    "# поделим предложения на уникальные слова\n",
    "for sentence in sentences:\n",
    "    vocab.update(sentence)\n",
    "\n",
    "vocab = list(vocab)\n",
    "print(vocab[:10], len(vocab))\n",
    "\n",
    "word_indx = {w: i for i, w in enumerate(vocab)}\n",
    "print(word_indx[\"я\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим положительные (реальные пары) и отрицательные примеры (случайные пары):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 'учусь'), ('я', 'в'), ('я', 'восьмом'), ('я', 'классе'), ('учусь', 'я'), ('учусь', 'в'), ('учусь', 'восьмом'), ('учусь', 'классе'), ('в', 'я'), ('в', 'учусь'), ('в', 'восьмом'), ('в', 'классе'), ('восьмом', 'я'), ('восьмом', 'учусь'), ('восьмом', 'в')]\n"
     ]
    }
   ],
   "source": [
    "positiv_pairs = []\n",
    "for sentence in sentences:\n",
    "    for i in range(len(sentence)):\n",
    "        target = sentence[i]\n",
    "        for j in range(max(0, i - L), min(i + L + 1, len(sentence))):\n",
    "            if i != j:\n",
    "                positiv_pairs.append((target, sentence[j]))\n",
    "\n",
    "print(positiv_pairs[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('я', 'Светлана'), ('я', 'мотнусь'), ('я', 'кубик?'), ('я', 'никому.'), ('учусь', 'нужду'), ('учусь', 'пеку'), ('учусь', 'ярче'), ('учусь', 'слоненком'), ('в', 'исправника'), ('в', 'интеллектуальных'), ('в', 'даш'), ('в', 'Лида.'), ('восьмом', 'ставили'), ('восьмом', 'лягушек'), ('восьмом', 'Мона')]\n"
     ]
    }
   ],
   "source": [
    "negative_pairs = []\n",
    "for target, context in positiv_pairs:\n",
    "    neg_context = random.choice(vocab)\n",
    "    while neg_context == context:\n",
    "        neg_context = random.choice(vocab)\n",
    "    negative_pairs.append((target, neg_context))\n",
    "\n",
    "print(negative_pairs[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим сбалансированный датасет, состоящий как из положительных, так и из отрицательных примеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61313 33584]\n",
      " [61313 45893]\n",
      " [61313 22698]\n",
      " [61313 65923]\n",
      " [33584 61313]] [1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [] # пары (индексы пар)\n",
    "Y = [] # метки (для реальных пар 1, для случайных 0)\n",
    "\n",
    "for target, contex in positiv_pairs:\n",
    "    X.append([word_indx[target], word_indx[contex]])\n",
    "    Y.append(1)\n",
    "\n",
    "for target, neg_context in negative_pairs:\n",
    "    X.append([word_indx[target], word_indx[neg_context]])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "print(X[:5], Y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс Word2Vec для реализации эмбедингов и обучения и создадим и обучим полносвязную нейронную сеть с одним слоем (dh = 500):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение модели с длиной эмбединга 100\n",
      "Обучение W2V\n",
      "Эпоха: 0, ошибка: 0.6931\n",
      "Эпоха: 1, ошибка: 0.6778\n",
      "Эпоха: 2, ошибка: 0.6379\n",
      "Эпоха: 3, ошибка: 0.6119\n",
      "Эпоха: 4, ошибка: 0.5925\n",
      "Эпоха: 5, ошибка: 0.5757\n",
      "Эпоха: 6, ошибка: 0.5595\n",
      "Слова, похожие на 'король':\n",
      "умру (сходство: 0.983)\n",
      "зиму? (сходство: 0.982)\n",
      "чью (сходство: 0.982)\n",
      "недавно (сходство: 0.982)\n",
      "жареные (сходство: 0.982)\n",
      "вожу (сходство: 0.982)\n",
      "программистом (сходство: 0.982)\n",
      "начал (сходство: 0.982)\n",
      "игр (сходство: 0.982)\n",
      "общение (сходство: 0.982)\n",
      "Создание и обучение нейросети\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksenia/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.0349 - loss: 9.7313 - val_accuracy: 0.0787 - val_loss: 8.3192\n",
      "Epoch 2/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 1s/step - accuracy: 0.0856 - loss: 7.2536 - val_accuracy: 0.1076 - val_loss: 8.2519\n",
      "Epoch 3/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 1s/step - accuracy: 0.1136 - loss: 6.4055 - val_accuracy: 0.1293 - val_loss: 8.4141\n",
      "Epoch 4/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 1s/step - accuracy: 0.1409 - loss: 5.7082 - val_accuracy: 0.1530 - val_loss: 8.8768\n",
      "Epoch 5/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - accuracy: 0.1776 - loss: 5.0135 - val_accuracy: 0.1788 - val_loss: 9.5124\n",
      "Epoch 6/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 1s/step - accuracy: 0.2222 - loss: 4.3254 - val_accuracy: 0.2033 - val_loss: 10.2372\n",
      "Epoch 7/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 1s/step - accuracy: 0.3006 - loss: 3.5979 - val_accuracy: 0.2229 - val_loss: 10.9035\n",
      "Epoch 8/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 1s/step - accuracy: 0.3995 - loss: 2.9433 - val_accuracy: 0.2422 - val_loss: 11.5813\n",
      "Epoch 9/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 1s/step - accuracy: 0.4983 - loss: 2.3682 - val_accuracy: 0.2572 - val_loss: 12.1541\n",
      "Epoch 10/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 1s/step - accuracy: 0.5861 - loss: 1.8831 - val_accuracy: 0.2636 - val_loss: 12.6178\n",
      "Epoch 11/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - accuracy: 0.6598 - loss: 1.5189 - val_accuracy: 0.2760 - val_loss: 13.0108\n",
      "Epoch 12/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 1s/step - accuracy: 0.7185 - loss: 1.2221 - val_accuracy: 0.2857 - val_loss: 13.3135\n",
      "Epoch 13/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 1s/step - accuracy: 0.7569 - loss: 1.0215 - val_accuracy: 0.2888 - val_loss: 13.5670\n",
      "Epoch 14/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 1s/step - accuracy: 0.7936 - loss: 0.8589 - val_accuracy: 0.2959 - val_loss: 13.7660\n",
      "Epoch 15/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.8241 - loss: 0.7234 - val_accuracy: 0.2979 - val_loss: 14.0273\n",
      "Обучение модели с длиной эмбединга 500\n",
      "Обучение W2V\n",
      "Эпоха: 0, ошибка: 0.6928\n",
      "Эпоха: 1, ошибка: 0.6690\n",
      "Эпоха: 2, ошибка: 0.6314\n",
      "Эпоха: 3, ошибка: 0.6075\n",
      "Эпоха: 4, ошибка: 0.5889\n",
      "Эпоха: 5, ошибка: 0.5723\n",
      "Эпоха: 6, ошибка: 0.5562\n",
      "Слова, похожие на 'король':\n",
      "болтать (сходство: 0.931)\n",
      "через (сходство: 0.931)\n",
      "поедешь (сходство: 0.931)\n",
      "компьютере (сходство: 0.931)\n",
      "чай (сходство: 0.930)\n",
      "идти (сходство: 0.930)\n",
      "нужна (сходство: 0.930)\n",
      "верю (сходство: 0.930)\n",
      "поговорить (сходство: 0.930)\n",
      "стать (сходство: 0.930)\n",
      "Создание и обучение нейросети\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksenia/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.0391 - loss: 9.6359 - val_accuracy: 0.0924 - val_loss: 8.1859\n",
      "Epoch 2/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 2s/step - accuracy: 0.1007 - loss: 7.0730 - val_accuracy: 0.1221 - val_loss: 8.0903\n",
      "Epoch 3/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.1342 - loss: 6.1172 - val_accuracy: 0.1501 - val_loss: 8.3406\n",
      "Epoch 4/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 2s/step - accuracy: 0.1766 - loss: 5.2576 - val_accuracy: 0.1877 - val_loss: 8.8728\n",
      "Epoch 5/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 2s/step - accuracy: 0.2471 - loss: 4.2669 - val_accuracy: 0.2150 - val_loss: 9.7836\n",
      "Epoch 6/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.3455 - loss: 3.2979 - val_accuracy: 0.2408 - val_loss: 10.8146\n",
      "Epoch 7/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.4964 - loss: 2.3986 - val_accuracy: 0.2653 - val_loss: 11.6872\n",
      "Epoch 8/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.6268 - loss: 1.7065 - val_accuracy: 0.2796 - val_loss: 12.5129\n",
      "Epoch 9/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.7198 - loss: 1.2428 - val_accuracy: 0.2898 - val_loss: 13.1136\n",
      "Epoch 10/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.7839 - loss: 0.9229 - val_accuracy: 0.2991 - val_loss: 13.4809\n",
      "Epoch 11/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 2s/step - accuracy: 0.8316 - loss: 0.7023 - val_accuracy: 0.3015 - val_loss: 13.7250\n",
      "Epoch 12/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.8585 - loss: 0.5730 - val_accuracy: 0.3049 - val_loss: 13.9193\n",
      "Epoch 13/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.8704 - loss: 0.4961 - val_accuracy: 0.3088 - val_loss: 14.0514\n",
      "Epoch 14/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 2s/step - accuracy: 0.8889 - loss: 0.4234 - val_accuracy: 0.3111 - val_loss: 14.1313\n",
      "Epoch 15/15\n",
      "\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 2s/step - accuracy: 0.8946 - loss: 0.3885 - val_accuracy: 0.3148 - val_loss: 14.1823\n",
      "Обучение модели с длиной эмбединга 1000\n",
      "Обучение W2V\n",
      "Эпоха: 0, ошибка: 0.6925\n",
      "Эпоха: 1, ошибка: 0.6660\n",
      "Эпоха: 2, ошибка: 0.6295\n",
      "Эпоха: 3, ошибка: 0.6061\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def show_similar_words(emb_matrix, word_indx, vocab, word, top_n=10):\n",
    "    if word not in word_indx:\n",
    "        print(f\"Слово '{word}' не найдено в словаре.\")\n",
    "        return\n",
    "    vec = emb_matrix[word_indx[word]].reshape(1, -1)\n",
    "    similarities = cosine_similarity(vec, emb_matrix)[0]\n",
    "    \n",
    "    # Отсортируем и выведем top_n похожих слов\n",
    "    similar_ids = np.argsort(-similarities)[:top_n + 1]\n",
    "    print(f\"Слова, похожие на '{word}':\")\n",
    "    for idx in similar_ids:\n",
    "        if vocab[idx] != word:\n",
    "            print(f\"{vocab[idx]} (сходство: {similarities[idx]:.3f})\")\n",
    "\n",
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embedding_dim, lambda_coef=0.001):\n",
    "        self.W = np.random.randn(vocab_size, embedding_dim) * lambda_coef\n",
    "        self.C = np.random.randn(vocab_size, embedding_dim) * lambda_coef\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -50, 50)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def train(self, positiv_pairs, negative_pairs, learning_rate=0.0005, epochs=7):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "\n",
    "            for target, context in positiv_pairs:\n",
    "                target_ind = word_indx[target]\n",
    "                context_ind = word_indx[context]\n",
    "\n",
    "                score = np.dot(self.W[target_ind], self.C[context_ind]) \n",
    "                prob = max(self.sigmoid(score), 1e-10)\n",
    "                loss = -np.log(prob)\n",
    "                total_loss += loss\n",
    "\n",
    "                grad = (1 - prob) * learning_rate\n",
    "                self.W[target_ind] += grad  * self.C[context_ind]\n",
    "                self.C[context_ind] += grad * self.W[target_ind]\n",
    "\n",
    "            for target, neg_context in negative_pairs:\n",
    "                target_ind = word_indx[target]\n",
    "                neg_ind = word_indx[neg_context]\n",
    "\n",
    "                score = np.dot(self.W[target_ind], self.C[neg_ind])\n",
    "                prob = self.sigmoid(-score)\n",
    "                loss = -np.log(prob)\n",
    "                total_loss += loss\n",
    "\n",
    "                grad = -prob * learning_rate\n",
    "                self.W[target_ind] += grad * self.C[neg_ind]\n",
    "                self.C[neg_ind] += grad * self.W[target_ind]\n",
    "            \n",
    "            print(f\"Эпоха: {epoch}, ошибка: {total_loss/len(positiv_pairs + negative_pairs):.4f}\")\n",
    "        \n",
    "        return self.W\n",
    "\n",
    "# Подготовка данных для нейросети\n",
    "def prepare_data(sentences, L):\n",
    "    X, y = [], []\n",
    "    for sentence in sentences:\n",
    "        for i in range(L, len(sentence)):\n",
    "            context = sentence[i-L:i]\n",
    "            target = sentence[i]\n",
    "            if all(word in word_indx for word in context) and target in word_indx:\n",
    "                X.append([word_indx[word] for word in context])\n",
    "                y.append(word_indx[target])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "dh = 500\n",
    "embedding_d = [100, 500, 1000]\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "for d in embedding_d:\n",
    "    print(f\"Обучение модели с длиной эмбединга {d}\")\n",
    "    \n",
    "    print(\"Обучение W2V\")\n",
    "    w2v = Word2Vec(len(vocab), d)\n",
    "    emb_matrix = w2v.train(positiv_pairs, negative_pairs)\n",
    "    \n",
    "    show_similar_words(emb_matrix, word_indx, vocab, \"король\")\n",
    "\n",
    "    X, y = prepare_data(sentences, L)\n",
    "    print(\"Создание и обучение нейросети\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(len(vocab), d, weights=[emb_matrix], input_length=L, trainable=True),\n",
    "        Flatten(),\n",
    "        Dense(dh, activation='relu'),\n",
    "        Dense(len(vocab), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        patience=2, \n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X, y, \n",
    "        epochs=15, \n",
    "        batch_size=512,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    models[d] = model\n",
    "    histories[d] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем нейронную сеть на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(models, vocab, word_indx, test_sentence, L, embedding_d):\n",
    "    # Токенизация и подготовка контекста\n",
    "    test_words = test_sentence.split()\n",
    "    \n",
    "    # Проверка, что контекст достаточной длины\n",
    "    if len(test_words) < L:\n",
    "        print(f\"Ошибка: предложение слишком короткое. Нужно минимум {L} слов, а получено {len(test_words)}\")\n",
    "        return\n",
    "    \n",
    "    # Берём последние L слов как контекст\n",
    "    context = test_words[-L:]\n",
    "    print(f\"\\nТестовый контекст: {' '.join(context)}\")\n",
    "    \n",
    "    # Преобразуем слова в индексы с обработкой неизвестных слов\n",
    "    test_indices = []\n",
    "    unk_token = \"<UNK>\"\n",
    "    \n",
    "    for word in context:\n",
    "        if word in word_indx:\n",
    "            test_indices.append(word_indx[word])\n",
    "        else:\n",
    "            print(f\"Слово '{word}' не найдено в словаре. Заменяю на {unk_token}\")\n",
    "            if unk_token in word_indx:\n",
    "                test_indices.append(word_indx[unk_token])\n",
    "            else:\n",
    "                print(f\"Ошибка: токен {unk_token} отсутствует в словаре\")\n",
    "                return\n",
    "    \n",
    "    # Предсказание для каждой модели\n",
    "    for d in embedding_d:\n",
    "        if d not in models:\n",
    "            print(f\"Модель с d={d} не найдена\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nПредсказание для модели с d={d}:\")\n",
    "        test_X = np.array([test_indices])\n",
    "        \n",
    "        try:\n",
    "            pred = models[d].predict(test_X, verbose=0)\n",
    "            predicted_idx = np.argmax(pred[0])\n",
    "            predicted_word = vocab[predicted_idx]\n",
    "            print(f\"Следующее слово: '{predicted_word}' (вероятность: {np.max(pred[0]):.2%})\")\n",
    "            \n",
    "            # Топ-5 вариантов\n",
    "            top_indices = np.argsort(-pred[0])[:5]\n",
    "            print(\"Топ-5 вариантов:\")\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                print(f\"{i+1}. {vocab[idx]} ({pred[0][idx]:.2%})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка предсказания: {str(e)}\")\n",
    "\n",
    "\n",
    "predict_next_word(models, vocab, word_indx, test_sentence, L, embedding_d)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
